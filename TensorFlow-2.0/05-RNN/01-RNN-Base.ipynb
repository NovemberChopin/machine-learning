{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=19, shape=(10, 4), dtype=float32, numpy=\n",
       "array([[ 0.04661176, -0.01172519, -0.01176087,  0.03441907],\n",
       "       [-0.0439463 ,  0.04628224, -0.03346934,  0.02707321],\n",
       "       [ 0.01686611,  0.01982513, -0.0047652 , -0.02863593],\n",
       "       [-0.0164413 ,  0.0465979 , -0.03928905, -0.04807564],\n",
       "       [-0.0037771 , -0.00973933,  0.04973311,  0.00934125],\n",
       "       [ 0.03580567,  0.04993408,  0.0094985 ,  0.04075507],\n",
       "       [ 0.04588017, -0.00286304,  0.00501217, -0.02724497],\n",
       "       [ 0.0308351 ,  0.02274876, -0.02220624,  0.01495382],\n",
       "       [ 0.01969836,  0.02583   , -0.00447027, -0.0243047 ],\n",
       "       [-0.04000853, -0.00137558, -0.01037323, -0.0041633 ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.range(10) # 生成 10 个单词的数字编码\n",
    "x = tf.random.shuffle(x) # 打散\n",
    "# 创建共 10 个单词,每个单词用长度为 4 的向量表示的层\n",
    "net = keras.layers.Embedding(10, 4)\n",
    "out = net(x) # 获取词向量\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'embedding/embeddings:0' shape=(10, 4) dtype=float32, numpy=\n",
       "array([[-0.0164413 ,  0.0465979 , -0.03928905, -0.04807564],\n",
       "       [ 0.01969836,  0.02583   , -0.00447027, -0.0243047 ],\n",
       "       [-0.04000853, -0.00137558, -0.01037323, -0.0041633 ],\n",
       "       [ 0.04661176, -0.01172519, -0.01176087,  0.03441907],\n",
       "       [ 0.04588017, -0.00286304,  0.00501217, -0.02724497],\n",
       "       [-0.0439463 ,  0.04628224, -0.03346934,  0.02707321],\n",
       "       [ 0.03580567,  0.04993408,  0.0094985 ,  0.04075507],\n",
       "       [-0.0037771 , -0.00973933,  0.04973311,  0.00934125],\n",
       "       [ 0.01686611,  0.01982513, -0.0047652 , -0.02863593],\n",
       "       [ 0.0308351 ,  0.02274876, -0.02220624,  0.01495382]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看 Embedding 层内部的 查询表 table\n",
    "net.embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SimpleRNNCell\n",
    "- 通过 SimpleRNNCell 层的使用,我们可以非常深入地理解循环神经网络前向运算的每个细节"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell = keras.layers.SimpleRNNCell(3)\n",
    "cell.build(input_shape=(None, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'kernel:0' shape=(4, 3) dtype=float32, numpy=\n",
       " array([[ 0.2711134 ,  0.64594245, -0.79462874],\n",
       "        [-0.08172673, -0.89395094, -0.46817762],\n",
       "        [-0.5066167 ,  0.4430678 , -0.24764663],\n",
       "        [-0.49155673,  0.3404919 , -0.4044373 ]], dtype=float32)>,\n",
       " <tf.Variable 'recurrent_kernel:0' shape=(3, 3) dtype=float32, numpy=\n",
       " array([[ 0.54365706,  0.8365746 , -0.06767614],\n",
       "        [-0.66276455,  0.3784318 , -0.64616764],\n",
       "        [-0.51495665,  0.39614686,  0.76018906]], dtype=float32)>,\n",
       " <tf.Variable 'bias:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 可以看到， SimpleRNNCell 内部维护了 3 个变量\n",
    "# kernel: W(xh)\n",
    "# recurrent_kernel: W(hh)\n",
    "# bias: 偏置 b\n",
    "cell.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 64) (4, 64)\n"
     ]
    }
   ],
   "source": [
    "# 示例 2\n",
    "# 初始化状态向量，用列表包裹，统一格式\n",
    "h0 = [tf.zeros([4, 64])]\n",
    "# [b, s, n] 输入 4 个长度为 80 的句子，每个单词向量长度为 100\n",
    "x = tf.random.normal([4, 80, 100])\n",
    "# 构建 cell\n",
    "cell = keras.layers.SimpleRNNCell(64)\n",
    "# 所有句子的自带一个单词\n",
    "xt = x[:, 0, :]\n",
    "# 前向计算\n",
    "out, h1 = cell(xt, h0)\n",
    "print(out.shape, h1[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**可以看到经过一个时间戳的计算后,输出和状态张量的 shape 都为 [b, h]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 由于 Memory 向量 h 并不是由 SimpleRNNCell 维护，用户需要自行初始化 h0 并记录每个时间戳上的 ht\n",
    "h = h0\n",
    "# 在序列长度的维度解开输入,得到 xt:[b,n]\n",
    "for xt in tf.unstack(x, axis=1):\n",
    "    out, h = cell(xt, h) # 前向计算，out 和 h 均被覆盖\n",
    "# 最后一个时间戳的输出变量 out 将作为网络的最终输出\n",
    "out = out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多层 SimpleRNNCell 网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 64)\n"
     ]
    }
   ],
   "source": [
    "x = tf.random.normal([4, 80, 100])\n",
    "xt = x[:, 0, :]\n",
    "cell0 = keras.layers.SimpleRNNCell(64)\n",
    "cell1 = keras.layers.SimpleRNNCell(64)\n",
    "h0 = [tf.zeros([4, 64])]\n",
    "h1 = [tf.zeros([4, 64])]\n",
    "# 在时间轴上面循环计算多次来实现整个网络的前向运算,每个时间戳上的输入 xt 首先通过\n",
    "# 第一层,得到输出 out0,再通过第二层,得到输出 out1\n",
    "for xt in tf.unstack(x, axis=1):\n",
    "    # xt 作为输入， 输出 out0\n",
    "    out0, h0 = cell0(xt, h0)\n",
    "    # 上一个 cell 的输出 out0 作为本 cell 的输入\n",
    "    out1, h1 = cell1(out0, h1)\n",
    "print(out1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SimpleRNN\n",
    "- 实际使用中,为了简便,不希望手动参与循环神经网络内部的计算过程\n",
    "- 通过 SimpleRNN层高层接口可以非常方便地帮助我们实现此目的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 64)\n"
     ]
    }
   ],
   "source": [
    "# 单层循环神经网络的运算\n",
    "layer = keras.layers.SimpleRNN(64) # 创建状态向量长度为 64 的 SimpleRNN 层\n",
    "x = tf.random.normal([4, 80, 100])\n",
    "out = layer(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 80, 64)\n"
     ]
    }
   ],
   "source": [
    "# 如果希望返回所有时间戳上的输出列表,可以设置 return_sequences=True 参数\n",
    "layer = keras.layers.SimpleRNN(64, return_sequences=True)\n",
    "out = layer(x)\n",
    "print(out.shape) \n",
    "# 中间维度的 80 即为时间戳维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([4, 64])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 多层 SimpleRNN \n",
    "# 每层都需要上一层在每个时间戳上面的状态输出,因此除了最末层以外,所有的 RNN 层\n",
    "# 都需要返回每个时间戳上面的状态输出,通过设置 return_sequences=True 来实现。\n",
    "net = keras.Sequential([\n",
    "    keras.layers.SimpleRNN(64, return_sequences=True),\n",
    "    keras.layers.SimpleRNN(64)\n",
    "])\n",
    "out = net(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN 情感分类问题实战"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1、加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,) 218 (25000,)\n",
      "(25000,) 68 (25000,)\n"
     ]
    }
   ],
   "source": [
    "batchsz = 128 # 批量大小\n",
    "total_words = 10000 # 词汇表大小 N_vocab\n",
    "max_review_len = 80 # 句子最大长度 s,大于的句子部分将截断,小于的将填充\n",
    "embedding_len = 100 # 词向量特征长度 n\n",
    "# 加载 IMDB 数据集,此处的数据采用数字编码,一个数字代表一个单词\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.imdb.load_data(num_words=total_words)\n",
    "# 打印输入的形状,标签的形状\n",
    "print(x_train.shape, len(x_train[0]), y_train.shape)\n",
    "print(x_test.shape, len(x_test[0]), y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train:[b, 80]\n",
    "# x_test: [b, 80]\n",
    "# 截断和填充句子，使得等长，此处长句子保留句子后面的部分，短句子在前面填充\n",
    "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=max_review_len)\n",
    "x_test = keras.preprocessing.sequence.pad_sequences(x_test, maxlen=max_review_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (25000, 80) tf.Tensor(1, shape=(), dtype=int64) tf.Tensor(0, shape=(), dtype=int64)\n",
      "x_test shape: (25000, 80)\n"
     ]
    }
   ],
   "source": [
    "# 构建数据集，打散，批量，并丢掉最后一个不够batchsz的batch\n",
    "db_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "db_train = db_train.shuffle(1000).batch(batchsz, drop_remainder=True)\n",
    "db_test = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "db_test = db_test.batch(batchsz, drop_remainder=True)\n",
    "print('x_train shape:', x_train.shape, tf.reduce_max(y_train), tf.reduce_min(y_train))\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2、网络模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 其中词向量编码为长度n = 100,RNN 的状态向量长度h = units参数\n",
    "# 分类网络完成 2 分类任务,故输出节点设置为 1。\n",
    "class MyRNN(keras.Model):\n",
    "    # Cell方式构建多层网络\n",
    "    def __init__(self, units):\n",
    "        super(MyRNN, self).__init__() \n",
    "        # 词向量编码 [b, 80] => [b, 80, 100]\n",
    "        self.embedding = layers.Embedding(total_words, embedding_len,\n",
    "                                          input_length=max_review_len)\n",
    "        # 构建RNN\n",
    "        self.rnn = keras.Sequential([\n",
    "            layers.SimpleRNN(units, dropout=0.5, return_sequences=True),\n",
    "            layers.SimpleRNN(units, dropout=0.5)\n",
    "        ])\n",
    "        # 构建分类网络，用于将CELL的输出特征进行分类，2分类\n",
    "        # [b, 80, 100] => [b, 64] => [b, 1]\n",
    "        self.outlayer = Sequential([\n",
    "        \tlayers.Dense(32),\n",
    "        \tlayers.Dropout(rate=0.5),\n",
    "        \tlayers.ReLU(),\n",
    "        \tlayers.Dense(1)])\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        x = inputs # [b, 80]\n",
    "        # embedding: [b, 80] => [b, 80, 100]\n",
    "        x = self.embedding(x)\n",
    "        # rnn cell compute,[b, 80, 100] => [b, 64]\n",
    "        x = self.rnn(x)\n",
    "        # 末层最后一个输出作为分类网络的输入: [b, 64] => [b, 1]\n",
    "        x = self.outlayer(x,training)\n",
    "        # p(y is pos|x)\n",
    "        prob = tf.sigmoid(x)\n",
    "\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3、训练与测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "195/195 [==============================] - 19s 98ms/step - loss: 0.6998 - accuracy: 0.4996 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "195/195 [==============================] - 19s 96ms/step - loss: 0.6932 - accuracy: 0.5054 - val_loss: 0.6928 - val_accuracy: 0.5050\n",
      "Epoch 3/10\n",
      "195/195 [==============================] - 19s 99ms/step - loss: 0.6936 - accuracy: 0.5139 - val_loss: 0.6927 - val_accuracy: 0.5113\n",
      "Epoch 4/10\n",
      "195/195 [==============================] - 17s 88ms/step - loss: 0.6505 - accuracy: 0.6046 - val_loss: 0.4753 - val_accuracy: 0.7867\n",
      "Epoch 5/10\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.4611 - accuracy: 0.8004 - val_loss: 0.4398 - val_accuracy: 0.8006\n",
      "Epoch 6/10\n",
      "195/195 [==============================] - 17s 90ms/step - loss: 0.3726 - accuracy: 0.8514 - val_loss: 0.4233 - val_accuracy: 0.8180\n",
      "Epoch 7/10\n",
      "195/195 [==============================] - 17s 85ms/step - loss: 0.3220 - accuracy: 0.8743 - val_loss: 0.4827 - val_accuracy: 0.8195\n",
      "Epoch 8/10\n",
      "195/195 [==============================] - 17s 85ms/step - loss: 0.2827 - accuracy: 0.8938 - val_loss: 0.4344 - val_accuracy: 0.8284\n",
      "Epoch 9/10\n",
      "195/195 [==============================] - 16s 84ms/step - loss: 0.2585 - accuracy: 0.9059 - val_loss: 0.4770 - val_accuracy: 0.8145\n",
      "Epoch 10/10\n",
      "195/195 [==============================] - 17s 85ms/step - loss: 0.2233 - accuracy: 0.9203 - val_loss: 0.5212 - val_accuracy: 0.8177\n",
      "195/195 [==============================] - 4s 21ms/step - loss: 0.5212 - accuracy: 0.8177\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5211831464217259, 0.81774837]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "units = 64 # RNN 状态向量长度 n\n",
    "epochs = 10 # 训练 epochs\n",
    "model = MyRNN(units) # 创建模型\n",
    "# 装配\n",
    "model.compile(optimizer = keras.optimizers.Adam(0.001),\n",
    "loss = keras.losses.BinaryCrossentropy(),\n",
    "metrics=['accuracy'])\n",
    "# 训练和验证\n",
    "model.fit(db_train, epochs=epochs, validation_data=db_test)\n",
    "# 测试\n",
    "model.evaluate(db_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2、LSTM\n",
    "### LSTMCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(139754918809272, 139754918809272, 139753464692576)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LSTM 的状态变量 List 有两个,即[ t , c t ]\n",
    "# 需要分别初始化,其中 List 第一个元素为 t ,第二个元素为c t 。\n",
    "x = tf.random.normal([2, 80, 100])\n",
    "xt = x[:, 0, :] # 得到第一个时间戳的输入\n",
    "cell = keras.layers.LSTMCell(64)\n",
    "# 初始化 List [h, c]\n",
    "state = [tf.zeros([2, 64]), tf.zeros([2, 64])]\n",
    "# 前向计算\n",
    "out, state = cell(xt, state)\n",
    "id(out), id(state[0]), id(state[1])\n",
    "# 可以看到,返回的输出 out 和 List 的第一个元素 t 的 id 是相同的\n",
    "# 这与基础的 RNN 初衷一致,都是为了格式的统一。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在序列长度维度上解开,循环送入 LSTM Cell 单元\n",
    "\n",
    "for xt in tf.unstack(x, axis=1):\n",
    "    # 前向计算\n",
    "    out, state = cell(xt, state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM 层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 64])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.random.normal([2, 80, 100])\n",
    "layer = keras.layers.LSTM(64)\n",
    "# 序列通过 LSTM 层,默认返回最后一个时间戳的输出 h\n",
    "out = layer(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 80, 64])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建 LSTM 层时,设置返回每个时间戳上的输出\n",
    "layer = layers.LSTM(64, return_sequences=True)\n",
    "out = layer(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 64])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 多层神经网络，通过 Sequential 容器包裹多层 LSTM 层\n",
    "# 并设置所有非末层网络 return_sequences=True\n",
    "# 这是因为非末层的 LSTM 层需要上一层在所有时间戳的输出作为输入\n",
    "net = keras.Sequential([\n",
    "    keras.layers.LSTM(64, return_sequences=True),\n",
    "    keras.layers.LSTM(64)\n",
    "])\n",
    "out = net(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM情感分类实战"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLSTM(keras.Model):\n",
    "    # Cell方式构建多层网络\n",
    "    def __init__(self, units):\n",
    "        super(MyLSTM, self).__init__() \n",
    "        # 词向量编码 [b, 80] => [b, 80, 100]\n",
    "        self.embedding = layers.Embedding(total_words, embedding_len,\n",
    "                                          input_length=max_review_len)\n",
    "        # 构建RNN\n",
    "        self.rnn = keras.Sequential([\n",
    "            layers.LSTM(units, dropout=0.5, return_sequences=True),\n",
    "            layers.LSTM(units, dropout=0.5)\n",
    "        ])\n",
    "        # 构建分类网络，用于将CELL的输出特征进行分类，2分类\n",
    "        # [b, 80, 100] => [b, 64] => [b, 1]\n",
    "        self.outlayer = Sequential([\n",
    "        \tlayers.Dense(32),\n",
    "        \tlayers.Dropout(rate=0.5),\n",
    "        \tlayers.ReLU(),\n",
    "        \tlayers.Dense(1)])\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        x = inputs # [b, 80]\n",
    "        # embedding: [b, 80] => [b, 80, 100]\n",
    "        x = self.embedding(x)\n",
    "        # rnn cell compute,[b, 80, 100] => [b, 64]\n",
    "        x = self.rnn(x)\n",
    "        # 末层最后一个输出作为分类网络的输入: [b, 64] => [b, 1]\n",
    "        x = self.outlayer(x,training)\n",
    "        # p(y is pos|x)\n",
    "        prob = tf.sigmoid(x)\n",
    "\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**训练与测试**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "195/195 [==============================] - 33s 170ms/step - loss: 0.5020 - accuracy: 0.7461 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "195/195 [==============================] - 29s 150ms/step - loss: 0.3248 - accuracy: 0.8697 - val_loss: 0.3807 - val_accuracy: 0.8364\n",
      "Epoch 3/10\n",
      "195/195 [==============================] - 28s 145ms/step - loss: 0.2661 - accuracy: 0.8959 - val_loss: 0.3761 - val_accuracy: 0.8349\n",
      "Epoch 4/10\n",
      "195/195 [==============================] - 29s 150ms/step - loss: 0.2292 - accuracy: 0.9121 - val_loss: 0.4270 - val_accuracy: 0.8308\n",
      "Epoch 5/10\n",
      "195/195 [==============================] - 28s 144ms/step - loss: 0.2004 - accuracy: 0.9227 - val_loss: 0.4907 - val_accuracy: 0.8244\n",
      "Epoch 6/10\n",
      "195/195 [==============================] - 29s 146ms/step - loss: 0.1699 - accuracy: 0.9356 - val_loss: 0.5506 - val_accuracy: 0.8211\n",
      "Epoch 7/10\n",
      "195/195 [==============================] - 28s 143ms/step - loss: 0.1526 - accuracy: 0.9432 - val_loss: 0.7607 - val_accuracy: 0.8154\n",
      "Epoch 8/10\n",
      "195/195 [==============================] - 29s 147ms/step - loss: 0.1342 - accuracy: 0.9490 - val_loss: 0.7571 - val_accuracy: 0.8173\n",
      "Epoch 9/10\n",
      "195/195 [==============================] - 31s 161ms/step - loss: 0.1169 - accuracy: 0.9569 - val_loss: 0.7044 - val_accuracy: 0.8154\n",
      "Epoch 10/10\n",
      "195/195 [==============================] - 29s 150ms/step - loss: 0.1028 - accuracy: 0.9622 - val_loss: 0.8196 - val_accuracy: 0.8169\n",
      "195/195 [==============================] - 7s 36ms/step - loss: 0.8196 - accuracy: 0.8169\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8195503366299165, 0.816867]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "units = 32 # RNN状态向量长度f\n",
    "epochs = 10 # 训练epochs\n",
    "\n",
    "model = MyLSTM(units)\n",
    "# 装配\n",
    "model.compile(optimizer = optimizers.Adam(0.001),\n",
    "              loss = losses.BinaryCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "# 训练和验证\n",
    "model.fit(db_train, epochs=epochs, validation_data=db_test)\n",
    "# 测试\n",
    "model.evaluate(db_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3、GRU使用方法\n",
    "### GRUCell 、GRU层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 64])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 初始化状态向量,GRU 只有一个\n",
    "h = [tf.zeros([2,64])]\n",
    "cell = layers.GRUCell(64) # 新建 GRU Cell,向量长度为 64\n",
    "# 在时间戳维度上解开,循环通过 cell\n",
    "for xt in tf.unstack(x, axis=1):\n",
    "    out, h = cell(xt, h)\n",
    "    \n",
    "# 输出形状\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 64])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = keras.Sequential([\n",
    "    layers.GRU(64, return_sequences=True),\n",
    "    layers.GRU(64)\n",
    "])\n",
    "out = net(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  15,  256,    4,    2,    7, 3766,    5,  723,   36,   71,   43,\n",
       "        530,  476,   26,  400,  317,   46,    7,    4,    2, 1029,   13,\n",
       "        104,   88,    4,  381,   15,  297,   98,   32, 2071,   56,   26,\n",
       "        141,    6,  194, 7486,   18,    4,  226,   22,   21,  134,  476,\n",
       "         26,  480,    5,  144,   30, 5535,   18,   51,   36,   28,  224,\n",
       "         92,   25,  104,    4,  226,   65,   16,   38, 1334,   88,   12,\n",
       "         16,  283,    5,   16, 4472,  113,  103,   32,   15,   16, 5345,\n",
       "         19,  178,   32], dtype=int32)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
